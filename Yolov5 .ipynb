{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80560209",
   "metadata": {},
   "source": [
    "# Code with Colour Box + Frame Caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dfcb436-3c75-4938-a9bd-0e23ab4cf4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  v7.0-321-g3742ab49 Python-3.11.9 torch-2.3.0+cu121 CUDA:0 (NVIDIA GeForce RTX 4080 Laptop GPU, 12282MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 346 layers, 76126356 parameters, 0 gradients, 109.9 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Selected cameras: [1, 0]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "import threading\n",
    "\n",
    "import pathlib\n",
    "temp = pathlib.PosixPath\n",
    "pathlib.PosixPath = pathlib.WindowsPath\n",
    "\n",
    "\n",
    "def draw_colored_box(image, color, thickness=5):\n",
    "    height, width = image.shape[:2]\n",
    "    start_point = (5, 5)  # Starting coordinate, (5, 5) indicates the top-left corner.\n",
    "    end_point = (width - 5, height - 5)  # Ending coordinate, (width-5, height-5) indicates the bottom-right corner.\n",
    "    cv2.rectangle(image, start_point, end_point, color, thickness)\n",
    "\n",
    "def video(frames, filename):\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "    out = cv2.VideoWriter(('capturedframe/'+filename), fourcc, 30.0, (frames[0].shape[1], frames[0].shape[0]))\n",
    "\n",
    "    for frame in frames:\n",
    "        out.write(frame)\n",
    "\n",
    "    out.release()\n",
    "    print('video stored')\n",
    "\n",
    "def put_stroked_text(image, text, position, font, font_scale, text_color, stroke_color, thickness, stroke_thickness):\n",
    "    # Draw the stroke (outline)\n",
    "    cv2.putText(image, text, position, font, font_scale, stroke_color, stroke_thickness)\n",
    "    # Draw the actual text\n",
    "    cv2.putText(image, text, position, font, font_scale, text_color, thickness)\n",
    "\n",
    "def handle_key_press(key_char, action):\n",
    "    if prev_keys.get(key_char, -1) != key and key == ord(key_char):\n",
    "        action()\n",
    "        prev_keys[key_char] = key\n",
    "    elif key != ord(key_char):\n",
    "        prev_keys[key_char] = -1\n",
    "\n",
    "\n",
    "r1 = []\n",
    "r2 = []\n",
    "f = 0\n",
    "\n",
    "# Load the YOLOv5 model\n",
    "combined_model = torch.hub.load(r\"C:\\Users\\iGauze\\yolov5\", 'custom', trust_repo=True, source='local',\n",
    "                                path=r\"C:\\Users\\iGauze\\Desktop\\Yolov5Exp\\exp47\\weights\\best.pt\", force_reload=True)\n",
    "\n",
    "combined_model.cuda()\n",
    "combined_model.amp = False\n",
    "combined_model.conf = 0.20  # Confidence threshold\n",
    "combined_model.classes = [0, 1]  # Class 0 for gauze, 1 for hand\n",
    "\n",
    "# Initialize variables for both cameras\n",
    "onscreenIn, onscreenOut, countIn, countOut, startTime, endTime, countPlay = 0, 0, 0, 0, time.time(), 0, 0\n",
    "frameCountIn, frameCountOut = 0, 0\n",
    "isPaused = False\n",
    "condition = 7\n",
    "\n",
    "\n",
    "capvalues = [0, 1, 2]\n",
    "selected_cameras = []\n",
    "\n",
    "# Function to switch to test videos\n",
    "def switch_to_test_videos():\n",
    "    return cv2.VideoCapture('Clean Test.mp4',cv2.CAP_DSHOW), cv2.VideoCapture('Dirty Test.mp4',cv2.CAP_DSHOW)\n",
    "\n",
    "# Select two cameras\n",
    "for i in range(2):  # We need to select two cameras\n",
    "    for cap_index in capvalues:\n",
    "        while True:\n",
    "            tempcap = cv2.VideoCapture(cap_index, cv2.CAP_DSHOW)\n",
    "            while True:\n",
    "                ret, frame = tempcap.read()\n",
    "                if not ret:\n",
    "                    print(f\"Camera {cap_index} not accessible.\")\n",
    "                    break  # Break out of the inner while loop if the camera is not accessible\n",
    "\n",
    "                cv2.imshow('frame', frame)\n",
    "                key = cv2.waitKey(10) & 0xFF\n",
    "\n",
    "                if key == ord('y'):\n",
    "                    tempcap.release()\n",
    "                    #cv2.destroyAllWindows()\n",
    "                    selected_cameras.append(cap_index)\n",
    "                    capvalues.remove(cap_index)\n",
    "                    k=0\n",
    "                    break  # Break out of the inner while loop and move to the next camera selection\n",
    "                if key == ord('n'):\n",
    "                    tempcap.release()\n",
    "                    #cv2.destroyAllWindows()\n",
    "                    break  # Break out of the inner while loop and try the next camera index\n",
    "                if key == ord('t'):\n",
    "                    tempcap.release()\n",
    "                    #cv2.destroyAllWindows()\n",
    "                    #cap1, cap2 = cv2.VideoCapture('Clean Test.mp4',cv2.CAP_DSHOW), cv2.VideoCapture('Dirty Test.mp4',cv2.CAP_DSHOW)\n",
    "                    k=1\n",
    "                    print(\"Switched to test videos.\")\n",
    "                    break  # Break out of the inner while loop and end camera selection\n",
    "\n",
    "            if key == ord('y') or key == ord('n') or key == ord('t'):\n",
    "                break  # Break out of the outer while loop to re-evaluate the next camera index\n",
    "\n",
    "        if key == ord('t') or key == ord('y'):\n",
    "            break  # Break out of the for loop if 't' is pressed to switch to test videos\n",
    "\n",
    "    if key == ord('t'):\n",
    "        break  # Break out of the main for loop if 't' is pressed to switch to test videos\n",
    "\n",
    "\n",
    "if k==0:\n",
    "    cap1 = cv2.VideoCapture(selected_cameras[0], cv2.CAP_DSHOW)\n",
    "    cap2 = cv2.VideoCapture(selected_cameras[1], cv2.CAP_DSHOW)\n",
    "elif k==1:\n",
    "    cap1 = cv2.VideoCapture('Clean Test.mp4')\n",
    "    cap2 = cv2.VideoCapture('Dirty Test.mp4')\n",
    "\n",
    "\n",
    "print(k)\n",
    "print(f\"Selected cameras: {selected_cameras}\")\n",
    "cv2.destroyAllWindows()\n",
    "# Initialize VideoCapture objects with the selected cameras\n",
    "\n",
    "\n",
    "# Initialize video capture for two cameras\n",
    "# cap1 = cv2.VideoCapture(r'Clean Test.mp4')  # First camera\n",
    "# cap2 = cv2.VideoCapture(r'Dirty Test.mp4')   # Second camera\n",
    "#cap1 = cv2.VideoCapture(selected_cameras[0])  # First camera\n",
    "#cap2 = cv2.VideoCapture(selected_cameras[1])   # Second camera\n",
    "\n",
    "# Display resolution and aspect ratio calculations\n",
    "display_width_per_camera = 1600 // 2\n",
    "display_height_per_camera = 900\n",
    "display_height = display_height_per_camera\n",
    "\n",
    "# Initialize the dictionary to track the previous key states\n",
    "prev_keys = {}\n",
    "\n",
    "framesToCapture = 100  # Change Value to change how long to capture when Pressing A for anomaly\n",
    "\n",
    "PAUSE_DURATION = 0.7  # in seconds\n",
    "UNPAUSE_DURATION = 0.1  # in seconds\n",
    "UNPAUSE_DURATION += PAUSE_DURATION  # Adding to how long the pause will be\n",
    "pause_timer = 0  # Timer variable to track the pause duration\n",
    "\n",
    "prev_red_frame_onscreenIn = None\n",
    "change_in_onscreenIn = 0\n",
    "change_in_onscreenOut = 0\n",
    "change_display_time = 0  # Variable to keep track of when to display the change\n",
    "\n",
    "\n",
    "            \n",
    "timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "folder_name = f\"captured_videos_{timestamp}\"\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "video_count = 0\n",
    "count = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame1 = cap1.read()\n",
    "    ret1, frame2 = cap2.read()\n",
    "\n",
    "    if not ret or not ret1:\n",
    "        print(\"Failed to capture frames from cameras\")\n",
    "        break\n",
    "    frame1 = cv2.rotate(frame1, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "    frame2 = cv2.rotate(frame2, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "\n",
    "    frame1 = cv2.resize(frame1, (display_width_per_camera, display_height_per_camera))\n",
    "    frame2 = cv2.resize(frame2, (display_width_per_camera, display_height_per_camera))\n",
    "    combined_frame = np.hstack((frame1, frame2))\n",
    "\n",
    "    results = combined_model(combined_frame)\n",
    "\n",
    "    prevOnScreenIn, prevOnScreenOut = onscreenIn, onscreenOut\n",
    "    a, b, handDetected = 0, 0, False\n",
    "    key = cv2.waitKey(10) & 0xFF\n",
    "\n",
    "    handle_key_press('1', lambda: globals().update(countIn=countIn + 1))\n",
    "    handle_key_press('2', lambda: globals().update(countIn=countIn - 1))\n",
    "    handle_key_press('3', lambda: globals().update(countOut=countOut + 1))\n",
    "    handle_key_press('4', lambda: globals().update(countOut=countOut - 1))\n",
    "    handle_key_press('c', lambda: globals().update(countOut=0, countIn=0))\n",
    "\n",
    "    for detection in results.xyxy[0]:\n",
    "        detected_class = detection[5].item()\n",
    "        x1, y1, x2, y2, conf, cls = detection\n",
    "        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "        if detected_class == 1:\n",
    "            if detection[0].item() < frame1.shape[1]-10:\n",
    "                a += 1\n",
    "            else:\n",
    "                b += 1\n",
    "        elif detected_class == 0:\n",
    "            handDetected = True\n",
    "\n",
    "        # Draw the bounding boxes\n",
    "        color = (255, 255, 0) if detected_class == 0 else (255, 0, 0)\n",
    "        cv2.rectangle(combined_frame, (x1, y1), (x2, y2), color, 2)  # Adjust thickness here\n",
    "\n",
    "        # Draw the text labels\n",
    "        label = f'{combined_model.names[int(cls)]} {conf:.2f}'\n",
    "        font_scale = 0.7  # Adjust font scale here\n",
    "        thickness = 1  # Adjust thickness here\n",
    "        cv2.putText(combined_frame, label, (x1, y1 - 2), cv2.FONT_HERSHEY_SIMPLEX, font_scale, color, thickness)\n",
    "\n",
    "    image = combined_frame\n",
    "\n",
    "    if handDetected and f == 0:\n",
    "        pause_timer = time.time()\n",
    "        isPaused = True\n",
    "        box_color = (0, 255, 255)\n",
    "        draw_colored_box(image, box_color)\n",
    "    elif pause_timer > 0 and time.time() - pause_timer >= PAUSE_DURATION and time.time() - pause_timer <= UNPAUSE_DURATION and f == 0:\n",
    "        isPaused = False\n",
    "        box_color = (0, 0, 255)\n",
    "        draw_colored_box(image, box_color)\n",
    "        \n",
    "    elif pause_timer > 0 and time.time() - pause_timer >= UNPAUSE_DURATION and f == 0:\n",
    "        isPaused = True\n",
    "        box_color = (0, 255, 0)\n",
    "        draw_colored_box(image, box_color)\n",
    "    elif f == 0:\n",
    "        isPaused = True\n",
    "        box_color = (0, 255, 255)\n",
    "        draw_colored_box(image, box_color)\n",
    "        \n",
    "    onscreenInTemp, onscreenOutTemp = a, b\n",
    "    if not isPaused:\n",
    "        onscreenIn, onscreenOut = a, b\n",
    "        if onscreenIn > prevOnScreenIn:\n",
    "            countIn += onscreenIn - prevOnScreenIn\n",
    "            change_in_onscreenIn = onscreenIn - prevOnScreenIn\n",
    "        if onscreenIn < prevOnScreenIn:\n",
    "            change_in_onscreenIn = onscreenIn - prevOnScreenIn\n",
    "        if onscreenOut > prevOnScreenOut:\n",
    "            countOut += onscreenOut - prevOnScreenOut\n",
    "            change_in_onscreenOut=  onscreenOut - prevOnScreenOut\n",
    "        if onscreenOut < prevOnScreenOut:   \n",
    "            change_in_onscreenOut=  onscreenOut - prevOnScreenOut\n",
    "            \n",
    "        countPlay = countIn - countOut - onscreenIn\n",
    "#         if prev_red_frame_onscreenIn is not None:\n",
    "#             change_in_onscreenIn = onscreenIn - prev_red_frame_onscreenIn\n",
    "#             change_display_time = time.time()  # Update the display time\n",
    "#         prev_red_frame_onscreenIn = onscreenIn\n",
    "\n",
    "    endTime = time.time()\n",
    "    fps = 1 / (endTime - startTime)\n",
    "    startTime = endTime\n",
    "    \n",
    "    #(image, text, position, font, font_scale, text_color, stroke_color, thickness, stroke_thickness)\n",
    "    put_stroked_text(image, f'On Screen = {onscreenInTemp}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                     1, (255, 255, 255), (0, 0, 0), 2, 8)\n",
    "    put_stroked_text(image, f'Total In = {countIn}', (10, 70), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                     1, (255, 255, 255), (0, 0, 0), 2, 8)\n",
    "    put_stroked_text(image, f'{change_in_onscreenIn}', (10, 110), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                     1, (255, 255, 255), (0, 0, 0), 2, 8)\n",
    "    put_stroked_text(image, f'On Screen = {onscreenOutTemp}', (display_width_per_camera + 10, 30),\n",
    "                     cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), (0, 0, 0), 2, 8)\n",
    "    put_stroked_text(image, f'Total Out = {countOut}', (display_width_per_camera + 10, 70),\n",
    "                     cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), (0, 0, 0), 2, 8)\n",
    "    put_stroked_text(image, f'{change_in_onscreenOut}', (display_width_per_camera + 10, 110), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                     1, (255, 255, 255), (0, 0, 0), 2, 8)\n",
    "    if countPlay == 0:\n",
    "        put_stroked_text(image, f'In Play = {countPlay}', (960 - 300, display_height - 50),\n",
    "                     cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), (0, 0, 0), 3, 8)\n",
    "    else:\n",
    "        put_stroked_text(image, f'In Play = {countPlay}', (960 - 300, display_height - 50),\n",
    "                     cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), (0, 0, 0), 3, 8)\n",
    "    put_stroked_text(image, f'FPS = {round(fps, 1)}', (960 - 100, display_height - 20),\n",
    "                     cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), (0, 0, 0), 2, 8)\n",
    "\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "        \n",
    "    if key == ord('a'):\n",
    "        f = 1\n",
    "        print('frames storing')\n",
    "        box_color = (0, 0, 255)\n",
    "        draw_colored_box(image, box_color)\n",
    "        isPaused = True\n",
    "\n",
    "    if f == 1:\n",
    "        if count <= framesToCapture:\n",
    "            r1.append(frame1)\n",
    "            r2.append(frame2)\n",
    "            count += 1\n",
    "            box_color = (0, 0, 255)\n",
    "            draw_colored_box(image, box_color)\n",
    "            text_size = cv2.getTextSize('CAPTURING FRAMES', cv2.FONT_HERSHEY_SIMPLEX, 4, 4)[0]\n",
    "            text_x = (image.shape[1] - text_size[0]) // 2\n",
    "            text_y = (image.shape[0] + text_size[1]) // 2\n",
    "            cv2.putText(image, 'CAPTURING FRAMES', (text_x, text_y),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 4, (0, 0, 255), 4)\n",
    "            isPaused = True\n",
    "        else:\n",
    "            f = 0\n",
    "            count = 0\n",
    "            video_count += 1\n",
    "            video(r1, f'{folder_name}/video_{video_count}_camera1.mp4')\n",
    "            video(r2, f'{folder_name}/video_{video_count}_camera2.mp4')\n",
    "            r1.clear()\n",
    "            r2.clear()\n",
    "\n",
    "    cv2.imshow('Gauze Detection', image)\n",
    "\n",
    "cap1.release()\n",
    "cap2.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2068e2bb-8c48-4ed7-8a6c-834b18b43906",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  v7.0-321-g3742ab49 Python-3.11.9 torch-2.3.0+cu121 CUDA:0 (NVIDIA GeForce RTX 4080 Laptop GPU, 12282MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 267 layers, 46113663 parameters, 0 gradients, 107.7 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switched to test videos.\n",
      "Selected cameras: []\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "import threading\n",
    "import collections\n",
    "import numpy as np\n",
    "import pathlib\n",
    "temp = pathlib.PosixPath\n",
    "pathlib.PosixPath = pathlib.WindowsPath\n",
    "\n",
    "\n",
    "def draw_colored_box(image, color, thickness=5):\n",
    "    height, width = image.shape[:2]\n",
    "    start_point = (5, 5)  # Starting coordinate, (5, 5) indicates the top-left corner.\n",
    "    end_point = (width - 5, height - 5)  # Ending coordinate, (width-5, height-5) indicates the bottom-right corner.\n",
    "    cv2.rectangle(image, start_point, end_point, color, thickness)\n",
    "\n",
    "def video(frames, filename):\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "    out = cv2.VideoWriter(filename, fourcc, 30.0, (frames[0].shape[1], frames[0].shape[0]))\n",
    "\n",
    "    for frame in frames:\n",
    "        out.write(frame)\n",
    "\n",
    "    out.release()\n",
    "    print('video stored')\n",
    "\n",
    "def put_stroked_text(image, text, position, font, font_scale, text_color, stroke_color, thickness, stroke_thickness):\n",
    "    # Draw the stroke (outline)\n",
    "    cv2.putText(image, text, position, font, font_scale, stroke_color, stroke_thickness)\n",
    "    # Draw the actual text\n",
    "    cv2.putText(image, text, position, font, font_scale, text_color, thickness)\n",
    "\n",
    "def handle_key_press(key_char, action):\n",
    "    if prev_keys.get(key_char, -1) != key and key == ord(key_char):\n",
    "        action()\n",
    "        prev_keys[key_char] = key\n",
    "    elif key != ord(key_char):\n",
    "        prev_keys[key_char] = -1\n",
    "\n",
    "\n",
    "r1 = []\n",
    "r2 = []\n",
    "f = 0\n",
    "\n",
    "# Load the YOLOv5 model\n",
    "combined_model = torch.hub.load(r\"C:\\Users\\iGauze\\yolov5\", 'custom', trust_repo=True, source='local',\n",
    "                                path=r\"C:\\Users\\iGauze\\Desktop\\exp38\\weights\\best.pt\", force_reload=True)\n",
    "\n",
    "combined_model.cuda()\n",
    "combined_model.amp = False\n",
    "combined_model.conf = 0.20  # Confidence threshold\n",
    "combined_model.classes = [0, 1]  # Class 0 for gauze, 1 for hand\n",
    "\n",
    "# Initialize variables for both cameras\n",
    "onscreenIn, onscreenOut, countIn, countOut, startTime, endTime, countPlay = 0, 0, 0, 0, time.time(), 0, 0\n",
    "frameCountIn, frameCountOut = 0, 0\n",
    "isPaused = False\n",
    "condition = 7\n",
    "\n",
    "\n",
    "capvalues = [0,1,2,3]\n",
    "selected_cameras = []\n",
    "\n",
    "\n",
    "# Select two cameras\n",
    "for i in range(2):  # We need to select two cameras\n",
    "    for cap_index in capvalues:\n",
    "        while True:\n",
    "            tempcap = cv2.VideoCapture(cap_index, cv2.CAP_DSHOW)\n",
    "            while True:\n",
    "                ret, frame = tempcap.read()\n",
    "                if not ret:\n",
    "                    print(f\"Camera {cap_index} not accessible.\")\n",
    "                    break  # Break out of the inner while loop if the camera is not accessible\n",
    "\n",
    "                cv2.imshow('frame', frame)\n",
    "                key = cv2.waitKey(10) & 0xFF\n",
    "\n",
    "                if key == ord('y'):\n",
    "                    tempcap.release()\n",
    "                    cv2.destroyAllWindows()\n",
    "                    selected_cameras.append(cap_index)\n",
    "                    capvalues.remove(cap_index)\n",
    "                    k=0\n",
    "                    break  # Break out of the inner while loop and move to the next camera selection\n",
    "                if key == ord('n'):\n",
    "                    tempcap.release()\n",
    "                    cv2.destroyAllWindows()\n",
    "                    break  # Break out of the inner while loop and try the next camera index\n",
    "                if key == ord('t'):\n",
    "                    tempcap.release()\n",
    "                    cv2.destroyAllWindows()\n",
    "                    cap1, cap2 = cv2.VideoCapture('Clean Test.mp4'), cv2.VideoCapture('Dirty Test.mp4')\n",
    "                    k=1\n",
    "                    print(\"Switched to test videos.\")\n",
    "                    break  # Break out of the inner while loop and end camera selection\n",
    "\n",
    "            if key == ord('y') or key == ord('n') or key == ord('t'):\n",
    "                break  # Break out of the outer while loop to re-evaluate the next camera index\n",
    "\n",
    "        if key == ord('t') or key == ord('y'):\n",
    "            break  # Break out of the for loop if 't' is pressed to switch to test videos\n",
    "\n",
    "    if key == ord('t'):\n",
    "        break  # Break out of the main for loop if 't' is pressed to switch to test videos\n",
    "\n",
    "print(f\"Selected cameras: {selected_cameras}\")\n",
    "\n",
    "# Initialize VideoCapture objects with the selected cameras\n",
    "if k==0:\n",
    "    cap1 = cv2.VideoCapture(selected_cameras[0], cv2.CAP_DSHOW)\n",
    "    cap2 = cv2.VideoCapture(selected_cameras[1], cv2.CAP_DSHOW)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize video capture for two cameras\n",
    "#cap1 = cv2.VideoCapture(r'Clean Test.mp4')  # First camera\n",
    "#cap2 = cv2.VideoCapture(r'Dirty Test.mp4')   # Second camera\n",
    "#cap1 = cv2.VideoCapture(selected_cameras[0])  # First camera\n",
    "#cap2 = cv2.VideoCapture(selected_cameras[1])   # Second camera\n",
    "\n",
    "# Display resolution and aspect ratio calculations\n",
    "display_width_per_camera = 1600 // 2\n",
    "display_height_per_camera = 900\n",
    "display_height = display_height_per_camera\n",
    "\n",
    "# Initialize the dictionary to track the previous key states\n",
    "prev_keys = {}\n",
    "\n",
    "framesToCapture = 100  # Change Value to change how long to capture when Pressing A for anomaly\n",
    "\n",
    "PAUSE_DURATION = 0.7  # in seconds\n",
    "UNPAUSE_DURATION = 0.1  # in seconds\n",
    "UNPAUSE_DURATION += PAUSE_DURATION  # Adding to how long the pause will be\n",
    "pause_timer = 0  # Timer variable to track the pause duration\n",
    "\n",
    "prev_red_frame_onscreenIn = None\n",
    "change_in_onscreenIn = 0\n",
    "change_in_onscreenOut = 0\n",
    "change_display_time = 0  # Variable to keep track of when to display the change\n",
    "\n",
    "skipper=0\n",
    "temptime11 = 14\n",
    "temptime12 = 14\n",
    "temptime21 = 14\n",
    "temptime22 = 14\n",
    "unpause_time = 0\n",
    "\n",
    "# Buffers for storing data\n",
    "buffer1 = collections.deque(maxlen=100)\n",
    "buffer2 = collections.deque(maxlen=100)\n",
    "last_change_time1=collections.deque(maxlen=100)\n",
    "last_change_time2=collections.deque(maxlen=100)\n",
    "\n",
    "'''\n",
    "# Parameters for anomaly detection\n",
    "window_size = 50\n",
    "fluctuation_threshold = 0.2\n",
    "consistent_threshold = 0.5 \n",
    "\n",
    "\n",
    "# Function to detect rapid fluctuations\n",
    "def detect_rapid_fluctuations(buffer, window_size, threshold):\n",
    "    if len(buffer) < window_size:\n",
    "        return False  # Not enough data to analyze\n",
    "    \n",
    "    window = np.array(buffer)[-window_size:]\n",
    "    mean = np.mean(window)\n",
    "    std_dev = np.std(window)\n",
    "    \n",
    "    # Detect if the standard deviation is high, indicating rapid fluctuations\n",
    "    if std_dev > threshold:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Function to detect consistent changes\n",
    "def detect_consistent_changes(buffer, consistent_threshold):\n",
    "    if len(buffer) < 2:\n",
    "        return False  # Not enough data to analyze\n",
    "    \n",
    "    # Detect if the current value is consistently different from the mean of the buffer\n",
    "    mean = np.mean(buffer)\n",
    "    if abs(buffer[-1] - mean) > consistent_threshold:\n",
    "        return True\n",
    "    return False\n",
    "'''\n",
    "\n",
    "timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "folder_name = f\"captured_videos_{timestamp}\"\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "video_count = 0\n",
    "count = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame1 = cap1.read()\n",
    "    ret1, frame2 = cap2.read()\n",
    "\n",
    "    if not ret or not ret1:\n",
    "        print(\"Failed to capture frames from cameras\")\n",
    "        break\n",
    "    frame1 = cv2.rotate(frame1, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "    frame2 = cv2.rotate(frame2, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "\n",
    "    frame1 = cv2.resize(frame1, (display_width_per_camera, display_height_per_camera))\n",
    "    frame2 = cv2.resize(frame2, (display_width_per_camera, display_height_per_camera))\n",
    "\n",
    "    combined_frame = np.hstack((frame1, frame2))\n",
    "\n",
    "    results = combined_model(combined_frame)\n",
    "\n",
    "    prevOnScreenIn, prevOnScreenOut = onscreenIn, onscreenOut\n",
    "    a, b, handDetected = 0, 0, False\n",
    "    key = cv2.waitKey(10) & 0xFF\n",
    "\n",
    "    handle_key_press('1', lambda: globals().update(countIn=countIn + 1))\n",
    "    handle_key_press('2', lambda: globals().update(countIn=countIn - 1))\n",
    "    handle_key_press('3', lambda: globals().update(countOut=countOut + 1))\n",
    "    handle_key_press('4', lambda: globals().update(countOut=countOut - 1))\n",
    "    handle_key_press('c', lambda: globals().update(countOut=0, countIn=0))\n",
    "\n",
    "\n",
    "    #Detection Section\n",
    "\n",
    "    for detection in results.xyxy[0]:\n",
    "        detected_class = detection[5].item()\n",
    "        x1, y1, x2, y2, conf, cls = detection\n",
    "        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "        if detected_class == 1:\n",
    "            if detection[0].item() < frame1.shape[1]-10:\n",
    "                a += 1\n",
    "            else:\n",
    "                b += 1\n",
    "        elif detected_class == 0:\n",
    "            handDetected = True\n",
    "\n",
    "        # Draw the bounding boxes\n",
    "        color = (255, 255, 0) if detected_class == 0 else (255, 0, 0)\n",
    "        cv2.rectangle(combined_frame, (x1, y1), (x2, y2), color, 2)  # Adjust thickness here\n",
    "\n",
    "        # Draw the text labels\n",
    "        label = f'{combined_model.names[int(cls)]} {conf:.2f}'\n",
    "        font_scale = 0.7  # Adjust font scale here\n",
    "        thickness = 1  # Adjust thickness here\n",
    "        cv2.putText(combined_frame, label, (x1, y1 - 2), cv2.FONT_HERSHEY_SIMPLEX, font_scale, color, thickness)\n",
    "\n",
    "    image = combined_frame\n",
    "       \n",
    "            \n",
    "    # Hand Detection and Traffic Light Section\n",
    "    \n",
    "    if handDetected and f == 0:\n",
    "        pause_timer = time.time()\n",
    "        isPaused = True\n",
    "        box_color = (0, 255, 255)\n",
    "        draw_colored_box(image, box_color)\n",
    "        buffer1.clear()\n",
    "        buffer2.clear()\n",
    "        #last_change_time1.clear()\n",
    "        #last_change_time2.clear()\n",
    "        skipper=1\n",
    "        \n",
    "    elif pause_timer > 0 and time.time() - pause_timer >= PAUSE_DURATION and time.time() - pause_timer <= UNPAUSE_DURATION and f == 0:\n",
    "        isPaused = False\n",
    "        box_color = (0, 0, 255)\n",
    "        draw_colored_box(image, box_color)\n",
    "        skipper=1\n",
    "        \n",
    "    elif pause_timer > 0 and time.time() - pause_timer >= UNPAUSE_DURATION and f == 0:\n",
    "        isPaused = True\n",
    "        box_color = (0, 255, 0)\n",
    "        draw_colored_box(image, box_color)\n",
    "        skipper=0\n",
    "    elif f == 0:\n",
    "        isPaused = True\n",
    "        box_color = (0, 255, 255)\n",
    "        draw_colored_box(image, box_color)\n",
    "        \n",
    "    onscreenInTemp, onscreenOutTemp = a, b\n",
    "\n",
    "    # Anomaly detection section\n",
    "    if not handDetected:\n",
    "        current_time = time.time()\n",
    "        \n",
    "        if onscreenInTemp != buffer1[-1] if buffer1 else None:\n",
    "            last_change_time1.append(current_time)\n",
    "        \n",
    "        if onscreenOutTemp != buffer2[-1] if buffer2 else None:\n",
    "            last_change_time2.append(current_time)\n",
    "    \n",
    "        buffer1.append(onscreenInTemp)\n",
    "        buffer2.append(onscreenOutTemp)\n",
    "        \n",
    "        try:  #Two try statements so they do not affect one another\n",
    "            temptime11 = current_time - last_change_time1[-1]\n",
    "            temptime12 = last_change_time1[-1] - last_change_time1[-2]\n",
    "        except:\n",
    "            why=1+1\n",
    "\n",
    "        try:\n",
    "            temptime21 = current_time - last_change_time2[-1]\n",
    "            temptime22 = last_change_time2[-1] - last_change_time2[-2]\n",
    "        except:\n",
    "            why=1+1\n",
    "            #temptime11 = temptime12 = temptime21 = temptime22 = 13\n",
    "            \n",
    "        anomalytime=2\n",
    "        \n",
    "        # Detect rapid fluctuations\n",
    "        if (temptime11 < anomalytime and temptime12 < anomalytime) or (temptime21 < anomalytime and temptime22 < anomalytime):\n",
    "            isPaused = True\n",
    "            box_color = (0, 0, 255)\n",
    "            draw_colored_box(image, box_color)\n",
    "            text_size = cv2.getTextSize('ANOMALY DETECTED', cv2.FONT_HERSHEY_SIMPLEX, 4, 4)[0]\n",
    "            text_x = (image.shape[1] - text_size[0]) // 2\n",
    "            text_y = (image.shape[0] + text_size[1]) // 2\n",
    "            cv2.putText(image, 'ANOMALY DETECTED', (text_x, text_y),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 4, (0, 0, 255), 4)\n",
    "        \n",
    "    if not handDetected and skipper == 0:\n",
    "        \n",
    "        if (temptime11 > 10 and temptime11 < 10.5) or (temptime21 > 10 and temptime21 < 10.5):\n",
    "            isPaused = False\n",
    "\n",
    "        elif temptime11 >= 10.5 or temptime21 >= 10.5:\n",
    "            isPaused = True\n",
    "\n",
    "   \n",
    "    '''\n",
    "    # Anomaly detection section\n",
    "    if not handDetected:\n",
    "        buffer1.append(onscreenInTemp)\n",
    "        buffer2.append(onscreenOutTemp)\n",
    "        \n",
    "        rapid_fluctuations1 = detect_rapid_fluctuations(buffer1, window_size, fluctuation_threshold)\n",
    "        rapid_fluctuations2 = detect_rapid_fluctuations(buffer2, window_size, fluctuation_threshold)\n",
    "        \n",
    "        consistent_changes1 = detect_consistent_changes(buffer1, consistent_threshold)\n",
    "        consistent_changes2 = detect_consistent_changes(buffer2, consistent_threshold)\n",
    "        \n",
    "        if rapid_fluctuations1 or rapid_fluctuations2:\n",
    "            isPaused = True\n",
    "            box_color = (0, 0, 255)\n",
    "            draw_colored_box(image, box_color)\n",
    "            text_size = cv2.getTextSize('ANOMALY DETECTED', cv2.FONT_HERSHEY_SIMPLEX, 4, 4)[0]\n",
    "            text_x = (image.shape[1] - text_size[0]) // 2\n",
    "            text_y = (image.shape[0] + text_size[1]) // 2\n",
    "            cv2.putText(image, 'ANOMALY DETECTED', (text_x, text_y),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 4, (0, 0, 255), 4)\n",
    "        elif consistent_changes1 or consistent_changes2:\n",
    "            isPaused = False\n",
    "        else:\n",
    "            isPaused = True\n",
    "        '''\n",
    "    if not isPaused:\n",
    "        onscreenIn, onscreenOut = a, b\n",
    "        if onscreenIn > prevOnScreenIn:\n",
    "            countIn += onscreenIn - prevOnScreenIn\n",
    "            change_in_onscreenIn = onscreenIn - prevOnScreenIn\n",
    "        if onscreenIn < prevOnScreenIn:\n",
    "            change_in_onscreenIn = onscreenIn - prevOnScreenIn\n",
    "        if onscreenOut > prevOnScreenOut:\n",
    "            countOut += onscreenOut - prevOnScreenOut\n",
    "            change_in_onscreenOut=  onscreenOut - prevOnScreenOut\n",
    "        if onscreenOut < prevOnScreenOut:   \n",
    "            change_in_onscreenOut=  onscreenOut - prevOnScreenOut\n",
    "            \n",
    "        countPlay = countIn - countOut - onscreenIn\n",
    "#         if prev_red_frame_onscreenIn is not None:\n",
    "#             change_in_onscreenIn = onscreenIn - prev_red_frame_onscreenIn\n",
    "#             change_display_time = time.time()  # Update the display time\n",
    "#         prev_red_frame_onscreenIn = onscreenIn\n",
    "\n",
    "    endTime = time.time()\n",
    "    fps = 1 / (endTime - startTime)\n",
    "    startTime = endTime\n",
    "   \n",
    "\n",
    "    # Text Section\n",
    "    \n",
    "    #(image, text, position, font, font_scale, text_color, stroke_color, thickness, stroke_thickness)\n",
    "    put_stroked_text(image, f'On Screen = {onscreenInTemp}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                     1, (255, 255, 255), (0, 0, 0), 2, 8)\n",
    "    put_stroked_text(image, f'Total In = {countIn}', (10, 70), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                     1, (255, 255, 255), (0, 0, 0), 2, 8)\n",
    "    put_stroked_text(image, f'{change_in_onscreenIn}', (10, 110), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                     1, (255, 255, 255), (0, 0, 0), 2, 8)\n",
    "    put_stroked_text(image, f'On Screen = {onscreenOutTemp}', (display_width_per_camera + 10, 30),\n",
    "                     cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), (0, 0, 0), 2, 8)\n",
    "    put_stroked_text(image, f'Total Out = {countOut}', (display_width_per_camera + 10, 70),\n",
    "                     cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), (0, 0, 0), 2, 8)\n",
    "    put_stroked_text(image, f'{change_in_onscreenOut}', (display_width_per_camera + 10, 110), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                     1, (255, 255, 255), (0, 0, 0), 2, 8)\n",
    "    if countPlay == 0:\n",
    "        put_stroked_text(image, f'In Play = {countPlay}', (960 - 300, display_height - 50),\n",
    "                     cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), (0, 0, 0), 3, 8)\n",
    "    else:\n",
    "        put_stroked_text(image, f'In Play = {countPlay}', (960 - 300, display_height - 50),\n",
    "                     cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), (0, 0, 0), 3, 8)\n",
    "    put_stroked_text(image, f'FPS = {round(fps, 1)}', (960 - 100, display_height - 20),\n",
    "                     cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), (0, 0, 0), 2, 8)\n",
    "\n",
    "\n",
    "    # Exit Section\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "    #Frame Storing Section\n",
    "    if key == ord('a'):\n",
    "        f = 1\n",
    "        print('frames storing')\n",
    "        box_color = (0, 0, 255)\n",
    "        draw_colored_box(image, box_color)\n",
    "        isPaused = True\n",
    "\n",
    "    # How it captures frames from prev section\n",
    "    if f == 1:\n",
    "        if count <= framesToCapture:\n",
    "            r1.append(frame1)\n",
    "            r2.append(frame2)\n",
    "            count += 1\n",
    "            box_color = (0, 0, 255)\n",
    "            draw_colored_box(image, box_color)\n",
    "            text_size = cv2.getTextSize('CAPTURING FRAMES', cv2.FONT_HERSHEY_SIMPLEX, 4, 4)[0]\n",
    "            text_x = (image.shape[1] - text_size[0]) // 2\n",
    "            text_y = (image.shape[0] + text_size[1]) // 2\n",
    "            cv2.putText(image, 'CAPTURING FRAMES', (text_x, text_y),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 4, (0, 0, 255), 4)\n",
    "            isPaused = True\n",
    "        else:\n",
    "            f = 0\n",
    "            count = 0\n",
    "            video_count += 1\n",
    "            video(r1, f'{folder_name}/video_{video_count}_camera1.mp4')\n",
    "            video(r2, f'{folder_name}/video_{video_count}_camera2.mp4')\n",
    "            r1.clear()\n",
    "            r2.clear()\n",
    "\n",
    "    cv2.imshow('Gauze Detection', image)\n",
    "\n",
    "cap1.release()\n",
    "cap2.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25e92f79-34c6-4643-8a09-5e7ac055d1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  v7.0-321-g3742ab49 Python-3.11.9 torch-2.3.0+cu121 CUDA:0 (NVIDIA GeForce RTX 4080 Laptop GPU, 12282MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 267 layers, 46113663 parameters, 0 gradients, 107.7 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "import threading\n",
    "from filterpy.kalman import KalmanFilter  # Make sure you have filterpy installed: pip install filterpy\n",
    "\n",
    "import pathlib\n",
    "temp = pathlib.PosixPath\n",
    "pathlib.PosixPath = pathlib.WindowsPath\n",
    "\n",
    "r1 = []\n",
    "r2 = []\n",
    "f = 0\n",
    "\n",
    "# Load the YOLOv5 model\n",
    "combined_model = torch.hub.load(r\"C:\\Users\\iGauze\\yolov5\", 'custom', trust_repo=True, source='local',\n",
    "                                path=r\"C:\\Users\\iGauze\\Desktop\\exp33\\weights\\best.pt\", force_reload=True)\n",
    "\n",
    "combined_model.cuda()\n",
    "combined_model.amp = False\n",
    "combined_model.conf = 0.20  # Confidence threshold\n",
    "combined_model.classes = [0, 1]  # Class 0 for gauze, 1 for hand\n",
    "\n",
    "# Initialize variables for both cameras\n",
    "onscreenIn, onscreenOut, countIn, countOut, startTime, endTime, countPlay = 0, 0, 0, 0, time.time(), 0, 0\n",
    "frameCountIn, frameCountOut = 0, 0\n",
    "isPaused = False\n",
    "condition = 7\n",
    "\n",
    "# Initialize video capture for two cameras\n",
    "cap1 = cv2.VideoCapture(r'C:\\Users\\\\iGauze\\\\Desktop\\\\Victor_Testing\\\\Yolov5\\\\Clean Test.mp4')  # First camera\n",
    "#cap2 = cv2.VideoCapture(r'C:\\Users\\\\iGauze\\\\Desktop\\\\Victor_Testing\\\\Yolov5\\\\Dirty Test.mp4')   # Second camera\n",
    "cap2 = cv2.VideoCapture(1)\n",
    "\n",
    "# Display resolution and aspect ratio calculations\n",
    "display_width_per_camera = 1920 // 2\n",
    "display_height_per_camera = int(display_width_per_camera * (9 / 16))\n",
    "display_height = display_height_per_camera\n",
    "\n",
    "# Initialize the dictionary to track the previous key states\n",
    "prev_keys = {}\n",
    "\n",
    "framesToCapture = 100  # Change Value to change how long to capture when Pressing A for anomaly\n",
    "\n",
    "PAUSE_DURATION = 0.7  # in seconds\n",
    "UNPAUSE_DURATION = 0.1  # in seconds\n",
    "UNPAUSE_DURATION += PAUSE_DURATION  # Adding to how long the pause will be\n",
    "pause_timer = 0  # Timer variable to track the pause duration\n",
    "\n",
    "prev_red_frame_onscreenIn = None\n",
    "change_in_onscreenIn = 0\n",
    "change_in_onscreenOut = 0\n",
    "change_display_time = 0  # Variable to keep track of when to display the change\n",
    "\n",
    "# Kalman filter tracking initialization\n",
    "class KalmanBoxTracker:\n",
    "    count = 0\n",
    "\n",
    "    def __init__(self, bbox):\n",
    "        self.kf = KalmanFilter(dim_x=7, dim_z=4)\n",
    "\n",
    "        self.kf.F = np.array([[1, 0, 0, 0, 1, 0, 0],\n",
    "                              [0, 1, 0, 0, 0, 1, 0],\n",
    "                              [0, 0, 1, 0, 0, 0, 1],\n",
    "                              [0, 0, 0, 1, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 1, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0],\n",
    "                              [0, 0, 0, 0, 0, 0, 1]])\n",
    "\n",
    "        self.kf.H = np.array([[1, 0, 0, 0, 0, 0, 0],\n",
    "                              [0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 1, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 1, 0, 0, 0]])\n",
    "\n",
    "        self.kf.R[2:, 2:] *= 10.0\n",
    "        self.kf.P[4:, 4:] *= 1000.0  # Give high uncertainty to the unobservable initial velocities\n",
    "        self.kf.P *= 10.0\n",
    "\n",
    "        self.kf.Q[-1, -1] *= 0.01\n",
    "        self.kf.Q[4:, 4:] *= 0.01\n",
    "\n",
    "        self.kf.x[:4] = self.convert_bbox_to_z(bbox)\n",
    "\n",
    "        self.time_since_update = 0\n",
    "        self.id = KalmanBoxTracker.count\n",
    "        KalmanBoxTracker.count += 1\n",
    "        self.history = []\n",
    "        self.hits = 0\n",
    "        self.hit_streak = 0\n",
    "        self.age = 0\n",
    "\n",
    "    def update(self, bbox):\n",
    "        self.time_since_update = 0\n",
    "        self.history = []\n",
    "        self.hits += 1\n",
    "        self.hit_streak += 1\n",
    "        self.kf.update(self.convert_bbox_to_z(bbox))\n",
    "\n",
    "    def predict(self):\n",
    "        if (self.kf.x[6] + self.kf.x[2]) <= 0:\n",
    "            self.kf.x[6] = 0\n",
    "        self.kf.predict()\n",
    "        self.age += 1\n",
    "        if self.time_since_update > 0:\n",
    "            self.hit_streak = 0\n",
    "        self.time_since_update += 1\n",
    "        self.history.append(self.convert_x_to_bbox(self.kf.x))\n",
    "        return self.history[-1]\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.convert_x_to_bbox(self.kf.x).flatten()\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_bbox_to_z(bbox):\n",
    "        w = bbox[2] - bbox[0]\n",
    "        h = bbox[3] - bbox[1]\n",
    "        x = bbox[0] + w / 2.0\n",
    "        y = bbox[1] + h / 2.0\n",
    "        s = w * h    # scale is just area\n",
    "        r = w / float(h)\n",
    "        return np.array([x, y, s, r]).reshape((4, 1))\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_x_to_bbox(x, score=None):\n",
    "        w = np.sqrt(x[2] * x[3])\n",
    "        h = x[2] / w\n",
    "        if score is None:\n",
    "            return np.array([x[0] - w / 2.0, x[1] - h / 2.0, x[0] + w / 2.0, x[1] + h / 2.0]).reshape((1, 4))\n",
    "        else:\n",
    "            return np.array([x[0] - w / 2.0, x[1] - h / 2.0, x[0] + w / 2.0, x[1] + h / 2.0, score]).reshape((1, 5))\n",
    "\n",
    "def compute_iou(box1, box2):\n",
    "    x1_min, y1_min, x1_max, y1_max = box1\n",
    "    x2_min, y2_min, x2_max, y2_max = box2\n",
    "\n",
    "    intersect_min_x = max(x1_min, x2_min)\n",
    "    intersect_min_y = max(y1_min, y2_min)\n",
    "    intersect_max_x = min(x1_max, x2_max)\n",
    "    intersect_max_y = min(y1_max, y2_max)\n",
    "\n",
    "    intersect_area = max(0, intersect_max_x - intersect_min_x) * max(0, intersect_max_y - intersect_min_y)\n",
    "    \n",
    "    box1_area = (x1_max - x1_min) * (y1_max - y1_min)\n",
    "    box2_area = (x2_max - x2_min) * (y2_max - y2_min)\n",
    "    \n",
    "    union_area = box1_area + box2_area - intersect_area\n",
    "    \n",
    "    iou = intersect_area / union_area\n",
    "    return iou\n",
    "\n",
    "def draw_colored_box(image, color, thickness=2):\n",
    "    height, width = image.shape[:2]\n",
    "    start_point = (5, 5)  # Starting coordinate, (5, 5) indicates the top-left corner.\n",
    "    end_point = (width - 5, height - 5)  # Ending coordinate, (width-5, height-5) indicates the bottom-right corner.\n",
    "    cv2.rectangle(image, start_point, end_point, color, thickness)\n",
    "\n",
    "def video(frames, filename):\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "    out = cv2.VideoWriter(filename, fourcc, 30.0, (frames[0].shape[1], frames[0].shape[0]))\n",
    "\n",
    "    for frame in frames:\n",
    "        out.write(frame)\n",
    "\n",
    "    out.release()\n",
    "    print('video stored')\n",
    "\n",
    "def put_stroked_text(image, text, position, font, font_scale, text_color, stroke_color, thickness, stroke_thickness):\n",
    "    # Draw the stroke (outline)\n",
    "    cv2.putText(image, text, position, font, font_scale, stroke_color, stroke_thickness)\n",
    "    # Draw the actual text\n",
    "    cv2.putText(image, text, position, font, font_scale, text_color, thickness)\n",
    "\n",
    "def handle_key_press(key_char, action):\n",
    "    if prev_keys.get(key_char, -1) != key and key == ord(key_char):\n",
    "        action()\n",
    "        prev_keys[key_char] = key\n",
    "    elif key != ord(key_char):\n",
    "        prev_keys[key_char] = -1\n",
    "\n",
    "timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "folder_name = f\"captured_videos_{timestamp}\"\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "video_count = 0\n",
    "count = 0\n",
    "min_hits = 7  # Minimum number of hits before displaying the tracker\n",
    "\n",
    "trackers = []\n",
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame1 = cap1.read()\n",
    "    ret1, frame2 = cap2.read()\n",
    "\n",
    "    if not ret or not ret1:\n",
    "        print(\"Failed to capture frames from cameras\")\n",
    "        break\n",
    "\n",
    "    frame1 = cv2.resize(frame1, (display_width_per_camera, display_height_per_camera))\n",
    "    frame2 = cv2.resize(frame2, (display_width_per_camera, display_height_per_camera))\n",
    "\n",
    "    combined_frame = np.hstack((frame1, frame2))\n",
    "\n",
    "    results = combined_model(combined_frame)\n",
    "\n",
    "    prevOnScreenIn, prevOnScreenOut = onscreenIn, onscreenOut\n",
    "    a, b, handDetected = 0, 0, False\n",
    "    key = cv2.waitKey(10) & 0xFF\n",
    "\n",
    "    handle_key_press('1', lambda: globals().update(countIn=countIn + 1))\n",
    "    handle_key_press('2', lambda: globals().update(countIn=countIn - 1))\n",
    "    handle_key_press('3', lambda: globals().update(countOut=countOut + 1))\n",
    "    handle_key_press('4', lambda: globals().update(countOut=countOut - 1))\n",
    "    handle_key_press('c', lambda: globals().update(countOut=0, countIn=0))\n",
    "\n",
    "    # Kalman filter tracking\n",
    "    detections = []\n",
    "    for detection in results.xyxy[0]:\n",
    "        detected_class = detection[5].item()\n",
    "        if detected_class == 1:  # Apply Kalman filter only to gauze detections\n",
    "            detections.append(detection[:4].cpu().numpy())\n",
    "            if detection[0].item() < frame1.shape[1]:\n",
    "                a += 1\n",
    "            else:\n",
    "                b += 1\n",
    "        elif detected_class == 0:\n",
    "            handDetected = True\n",
    "\n",
    "    # Update Kalman trackers\n",
    "    trackers_to_remove = []\n",
    "    for tracker in trackers:\n",
    "        tracker.predict()\n",
    "        if tracker.time_since_update > 1:\n",
    "            trackers_to_remove.append(tracker)\n",
    "\n",
    "    trackers = [t for t in trackers if t not in trackers_to_remove]\n",
    "\n",
    "    for det in detections:\n",
    "        matched = False\n",
    "        for tracker in trackers:\n",
    "            iou = compute_iou(det, tracker.get_state())\n",
    "            if iou > 0.9:\n",
    "                tracker.update(det)\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            trackers.append(KalmanBoxTracker(det))\n",
    "\n",
    "    # Manually draw bounding boxes and labels\n",
    "    image = combined_frame\n",
    "    for detection in results.xyxy[0]:\n",
    "        x1, y1, x2, y2, conf, cls = detection\n",
    "        if int(cls) in combined_model.classes:\n",
    "            color = (0, 255, 0) if int(cls) == 0 else (0, 0, 255)\n",
    "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "            cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n",
    "            label = f'{combined_model.names[int(cls)]} {conf:.2f}'\n",
    "            font_scale = 0.5\n",
    "            thickness = 1\n",
    "            cv2.putText(image, label, (x1, y1 - 2), cv2.FONT_HERSHEY_SIMPLEX, font_scale, color, thickness)\n",
    "\n",
    "    if handDetected and f == 0:\n",
    "        pause_timer = time.time()\n",
    "        isPaused = True\n",
    "        box_color = (0, 255, 255)\n",
    "        draw_colored_box(image, box_color)\n",
    "    elif pause_timer > 0 and time.time() - pause_timer >= PAUSE_DURATION and time.time() - pause_timer <= UNPAUSE_DURATION and f == 0:\n",
    "        isPaused = False\n",
    "        box_color = (0, 0, 255)\n",
    "        draw_colored_box(image, box_color)\n",
    "    elif pause_timer > 0 and time.time() - pause_timer >= UNPAUSE_DURATION and f == 0:\n",
    "        isPaused = True\n",
    "        box_color = (0, 255, 0)\n",
    "        draw_colored_box(image, box_color)\n",
    "    elif f == 0:\n",
    "        isPaused = True\n",
    "        box_color = (0, 255, 255)\n",
    "        draw_colored_box(image, box_color)\n",
    "    onscreenInTemp, onscreenOutTemp = a, b\n",
    "    if not isPaused:\n",
    "        onscreenIn, onscreenOut = a, b\n",
    "        if onscreenIn > prevOnScreenIn:\n",
    "            countIn += onscreenIn - prevOnScreenIn\n",
    "            change_in_onscreenIn = onscreenIn - prevOnScreenIn\n",
    "        if onscreenIn < prevOnScreenIn:\n",
    "            change_in_onscreenIn = onscreenIn - prevOnScreenIn\n",
    "        if onscreenOut > prevOnScreenOut:\n",
    "            countOut += onscreenOut - prevOnScreenOut\n",
    "            change_in_onscreenOut = onscreenOut - prevOnScreenOut\n",
    "        if onscreenOut < prevOnScreenOut:\n",
    "            change_in_onscreenOut = onscreenOut - prevOnScreenOut\n",
    "        countPlay = countIn - countOut - onscreenIn\n",
    "\n",
    "    # Display only trackers that have been updated at least `min_hits` times\n",
    "    for tracker in trackers:\n",
    "        if tracker.hits >= min_hits:\n",
    "            pred_bbox = tracker.get_state()\n",
    "            x1, y1, x2, y2 = int(pred_bbox[0]), int(pred_bbox[1]), int(pred_bbox[2]), int(pred_bbox[3])\n",
    "            cv2.rectangle(image, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "\n",
    "    endTime = time.time()\n",
    "    fps = 1 / (endTime - startTime)\n",
    "    startTime = endTime\n",
    "\n",
    "    put_stroked_text(image, f'On Screen = {onscreenInTemp}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                     1, (255, 255, 255), (0, 0, 0), 2, 8)\n",
    "    put_stroked_text(image, f'Total In = {countIn}', (10, 70), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                     1, (255, 255, 255), (0, 0, 0), 2, 8)\n",
    "    put_stroked_text(image, f'{change_in_onscreenIn}', (10, 110), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                     1, (255, 255, 255), (0, 0, 0), 2, 8)\n",
    "    put_stroked_text(image, f'On Screen = {onscreenOutTemp}', (display_width_per_camera + 10, 30),\n",
    "                     cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), (0, 0, 0), 2, 8)\n",
    "    put_stroked_text(image, f'Total Out = {countOut}', (display_width_per_camera + 10, 70),\n",
    "                     cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), (0, 0, 0), 2, 8)\n",
    "    put_stroked_text(image, f'{change_in_onscreenOut}', (display_width_per_camera + 10, 110), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                     1, (255, 255, 255), (0, 0, 0), 2, 8)\n",
    "    put_stroked_text(image, f'In Play = {countPlay}', (960 - 100, display_height - 50),\n",
    "                     cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), (0, 0, 0), 2, 8)\n",
    "    put_stroked_text(image, f'FPS = {round(fps, 1)}', (960 - 100, display_height - 20),\n",
    "                     cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), (0, 0, 0), 2, 8)\n",
    "\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "    if key == ord('a'):\n",
    "        f = 1\n",
    "        print('frames storing')\n",
    "        box_color = (0, 0, 255)\n",
    "        draw_colored_box(image, box_color)\n",
    "        isPaused = True\n",
    "\n",
    "    if f == 1:\n",
    "        if count <= framesToCapture:\n",
    "            r1.append(frame1)\n",
    "            r2.append(frame2)\n",
    "            count += 1\n",
    "            box_color = (0, 0, 255)\n",
    "            draw_colored_box(image, box_color)\n",
    "            text_size = cv2.getTextSize('CAPTURING FRAMES', cv2.FONT_HERSHEY_SIMPLEX, 4, 4)[0]\n",
    "            text_x = (image.shape[1] - text_size[0]) // 2\n",
    "            text_y = (image.shape[0] + text_size[1]) // 2\n",
    "            cv2.putText(image, 'CAPTURING FRAMES', (text_x, text_y),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 4, (0, 0, 255), 4)\n",
    "            isPaused = True\n",
    "        else:\n",
    "            f = 0\n",
    "            count = 0\n",
    "            video_count += 1\n",
    "            video(r1, f'{folder_name}/video_{video_count}_camera1.mp4')\n",
    "            video(r2, f'{folder_name}/video_{video_count}_camera2.mp4')\n",
    "            r1.clear()\n",
    "            r2.clear()\n",
    "\n",
    "    cv2.imshow('Gauze Detection', image)\n",
    "\n",
    "cap1.release()\n",
    "cap2.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3f073b-cea4-462c-8f20-64a2efde1899",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
